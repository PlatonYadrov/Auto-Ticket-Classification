train:
  # Training parameters (optimized for BERT fine-tuning)
  max_epochs: 15
  batch_size: 16
  learning_rate: 1.0e-5  # Lower LR for stable fine-tuning
  weight_decay: 0.01
  warmup_ratio: 0.15  # More warmup for BERT
  gradient_clip_val: 1.0

  # Scheduler
  scheduler: "linear_warmup"

  # Early stopping (monitor F1, not loss)
  early_stopping: true
  early_stopping_patience: 5  # More patience
  early_stopping_metric: "val_f1_macro"  # F1 is better indicator
  early_stopping_mode: "max"

  # Checkpointing
  checkpoint_dir: "checkpoints"
  save_top_k: 1
  checkpoint_metric: "val_f1_macro"
  checkpoint_mode: "max"

  # Hardware
  # "auto" will use MPS on Apple Silicon, CUDA on NVIDIA, CPU otherwise
  accelerator: "auto"
  devices: 1
  # Use 32 for MPS compatibility (16-mixed not fully supported on MPS)
  precision: 32

  # Multi-task loss weights
  topic_loss_weight: 1.0
  priority_loss_weight: 1.0

  # Class weighting for imbalanced data
  use_class_weights: true

  # Tokenizer settings
  max_length: 256
  padding: "max_length"
  truncation: true

  # DataLoader settings
  # num_workers: 0 for MPS (multiprocessing issues), increase for CUDA
  num_workers: 0
  # pin_memory: false for MPS, true for CUDA
  pin_memory: false
